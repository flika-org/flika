{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to detect subcellular Ca<sup>2+</sup> signals using [Flika](http://flika-org.github.io/) \n",
    "by Kyle Ellefsen, January 17, 2017\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is relevant for researchers who record subcellular Ca<sup>2+</sup> signals using fluorescent Ca<sup>2+</sup> indicators.  This algorithm automates the detection and analysis of these brief Ca2+ signals.  This is not a tutorial for researchers looking to analyze global Ca<sup>2+</sup> signals, although I'm planning on creating a tutorial for that, and many of the same concepts apply.\n",
    "\n",
    "In this problem, we will use the threshold-cluster algorithm.  This algorithm is not published, but is modified from the algorithm published in [Ellefsen et al, 2014](http://dx.doi.org/10.1016/j.ceca.2014.06.003).  When using this algorithm, please cite that paper.  The threshold-cluster algorithm can be conceptually divided into two steps.  The first step preprocesses the raw movie and creates three movies.  The second step performs analysis on those movies according to user defined parameters.  I'll break this tutorial into those two steps.  \n",
    "\n",
    "Using this algorithm involves reading and writing code.  Although this algorithm can all be done using Flika's GUI tools (opening windows, clicking buttons, etc.), it is much more efficient for a researcher to write a python file that will automatically perform all these operations.  Therefore, in this tutorial, I will be describing the code you'll need to use in each step.  If you do not know how to code in python, I recommend downloading [anaconda](https://www.continuum.io/downloads), which contains python and many python packages.  Then download and launch the Community version of the IDE [PyCharm](https://www.jetbrains.com/pycharm/download), and from within PyCharm, either run [Flika.py](https://github.com/flika-org/flika) or, to execute code in the same console running Flika, enter [this code](https://gist.github.com/kyleellefsen/9d1f72a393aa7f29cb1a37182fa8ed41) in a console.  All these commands will be executed in the same console that Flika is running in. If you have questions about how to do that, email me (kellefse at uci dot edu) and I'll help you with setting up.\n",
    "\n",
    "![step flowchart](images/flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "There are three movies we want to generate as part of the preprocessing step.\n",
    "1. $F/F_0$ Movie\n",
    "2. Normalized Movie\n",
    "3. Blurred Movie\n",
    "\n",
    "First, let's generate the $F/F_0$ movie. Each value of the $F/F_0$ movie represents a percent change from baseline fluoresences.  A value of 1 indicates that pixel is currently exactly at baseline fluoresence.  A value of 1.14 indicates a 14% increase above baseline fluorsence. To generate this movie, we will first open our file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "open_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will subtract the camera's black level.  The black level is the value of a pixel when 0 photons are detected.  There are two common ways to measure black level.  You could turn the laser off at the beginning of your record, and switch it on when you've recorded enough frames to accurately determine the black level.  Or you could measure the values in a black area of your field of view.  Let's say our black level is 120."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subtract(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations in Flika are performed on the current window, which is either the window that was most recently created or the window that was last clicked on.  \n",
    "\n",
    "Now, for each pixel, we want the percent change from baseline.  To calculate this, we need a measure of baseline fluorescence, during which there is no Ca<sup>2+</sup> activity.  Let's suppose that our cell is quiescent from frame 0 to frame 30.  Then we will take the average image from frame 0 to 30, and divide every image in the movie by that average image.  In Flika, this is performed in a single command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_window=ratio(150,300,'average'); #ratio(first_frame, nFrames, ratio_type), now we are in F/F0\n",
    "data_window.setName('Data Window dF/F0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we set the variable 'data_window'.  The output of each Flika command returns the new window created by the command, which we can assign to a variable for later use.  We now have our data_window.  \n",
    "\n",
    "The normalized movie should have several special properties that will enable automated signal detection.  The noise for each pixel should be gaussian distributed with a mean of 0 and a standard deviation of 1 ($\\mu = 0; \\sigma =1$).  Since data files won't start off with those properties, we need to transform them. \n",
    "\n",
    "Since we are concerned with only local signals, sometimes we want to remove global signals or other slow noise.  We can exploit the fact that global signals typically have a much slower timecourse than brief local signals, and use a temporal filter to remove slow changes.  We will use the butterworth filter.  In order to determine what parameters we want to use, let's use the the butterworth filter GUI.  Right click and drag on the $F/F_0$ movie window to create an ROI.  Right click in the center of that ROI, and plot the trace.  Then, in the main Flika window, select Process->Filters->Butterworth Filter.  Make sure the Preview checkbox is checked.  Adjust the Low Cutoff Frequency until the global signals are removed, but the local signals are still visible.  Then press the 'Cancel' button and use the parameters you found to perform the filter.  Sometimes this filter can take a few minutes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "butterworth_filter(1,.03,1,keepSourceWindow=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't need to filter your data, just subtract 1 from each value so that the noise is centered around 0.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subtract(1) #Don't do this if you used the filter above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now our movie should have its noise centered around 0.  We want the standard deviation of our noise to be equal to 1, so we'll find a quiescent section of our movie and ratio by the standard deviation, similar to when we ratioed by the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratio(100,100,'standard deviation')  #ratio(first_frame, nFrames, ratio_type), now the standard devation of the noise is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our butterworth filter created artifacts at the beginning and end of the movie.  Let's eliminate those by setting the values at the beginning and end of the movie to 0. 'mt' is a variable that represents number of frames of our movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_window=set_value(0,0,100) #to get rid of butterworth artifact at the beginning of the movie\n",
    "norm_window=set_value(0,mt-150,mt-1) #to get rid of butterworth artifact at the end of the movie\n",
    "norm_window.setName('Normalized Window')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We're most of the way there.  We have our $F/F_0$ Movie and our Normalized Movie.  The last movie we need is the Blurred Movie.  All we need to do is run the gaussian blur command. We will use the 'norm_edges = True' argument to reduce the amplitude of the values at the edge of the movie, otherwise the standard deviation of these pixels is greater than at the center of the movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blurred_window=gaussian_blur(2, norm_edges = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Clustering\n",
    "The second half of the algorithm requires only those three movies we created earlier:  \n",
    "1. $F/F_0$ Movie\n",
    "2. Normalized Movie\n",
    "3. Blurred Movie\n",
    "\n",
    "\n",
    "\n",
    "We pass these three movies into the 'threshold_cluster()' function, along with some constants, and the algorithm detects subcellular signals! And like many black boxes, for it to work correctly, we have to know what it's doing under the hood.  So what does this function do? For this half of the tutorial, it is helpful to work along with an example.  Enter this code into your IDE's interpreter, after setting ```flika_dir``` to the location of your flika directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys; flika_dir = os.path.join(os.path.expanduser('~'),'Documents', 'GitHub', 'flika'); sys.path.append(flika_dir); from flika import *; start_flika()\n",
    "from plugins.detect_puffs.threshold_cluster import threshold_cluster\n",
    "from plugins.detect_puffs.puff_simulator.puff_simulator import simulate_puffs\n",
    "simulate_puffs(nFrames=1000,nPuffs=20)\n",
    "baseline = -5  # This is the mean pixel value in the absence of photons.\n",
    "subtract(baseline)\n",
    "data_window=ratio(0, 30, 'average')  # ratio(first_frame, nFrames, ratio_type). Now we are in F/F0\n",
    "data_window.setName('Data Window (F/F0)')\n",
    "norm_image = data_window.image - 1\n",
    "norm_window = Window(norm_image)\n",
    "norm_window.setName('Normalized Window')\n",
    "blurred_window = gaussian_blur(2, norm_edges=True, keepSourceWindow=True)\n",
    "blurred_window.setName('Blurred Window')\n",
    "threshold_cluster(data_window, blurred_window, blurred_window, blur_thresh=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm GUI\n",
    "If you ran the above commands, you should have a window that looks like this:\n",
    "\n",
    "![algorithm_gui](images/algorithm_gui.png)\n",
    "\n",
    "We will be working left to right through the tabs.  First, we need to select which pixels we will be feeding into our clustering algorithm (explained in the next step).  To do this we will set a threshold value.  Pixels above this value will go into the next step.  Pick a value that is lower than the peak of your smallest event.  Don't pick a value that is too low, however, because this can dramatically increase the computation time for the next step.  Start with higher values and work down if you want to detect smaller events. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "The clustering algorithm is based on the one published by [Rodriguez and Laio, 2014](http://doi.org/10.1126/science.1242072). The basic idea is simple.  We have a bunch of pixels with values in our blurred movie.  For each pixel, we find nearest pixel with a higher value.  For pixels that are on the edge of an event, those pixels with higher values will be close.  See the little cartoon I made below.  The green arrows show pixels pointing to the pixel identified as having a higher value.\n",
    "![clustering cartoon](images/clustering_cartoon.PNG)\n",
    "One arrow stands out.  One pixel near the center of the cluster has the highest value, so the arrow from it is much much longer than any of the other arrows in the cluster. In general, all clusers should have this property: one pixel near the center should be pointing to a pixel that is pointing far away, every other pixel in the cluster will have small arrows.  From this principle, we can find clusters.  All we need to do is identify the pixels that have abnormally high distances to the closest pixel of higher value.  These pixels should also have an abnormally high density, since many isolated pixels will also have abnormally high arrows.  I've simulated a few signals and plotted the density of each true pixel vs the smallest distance to a denser point.\n",
    "\n",
    "![Density vs Distance](images/density_vs_distance.PNG)\n",
    "\n",
    "You can see that there are several points that have a high density and distance (20 in this example).  These are the centers of the clusters.  The clustering algorithm considers all pixels that point to these centers to be part of these clusters.  Circle the points at the top right of the graph, and Flika will output a movie where each cluster is colored differently.  This way you can be sure that you've clustered how you would like.  Include some pixels in the top left also; even though these are not cluster centers, we don't want them clustered with our true events, because they are too far away from higher points.  It doesn't matter very much where on the y axis you slice.  A lot of people get hung up on this.  Try slicing in different ways and see how much difference it makes.\n",
    "\n",
    "![Density vs Distance (circled)](images/density_vs_distance_circled.PNG)\n",
    "\n",
    "The pixels that are considered 'centers' will be colored green after circling.\n",
    "\n",
    "![clusters](images/clusters.PNG)\n",
    "\n",
    "If you don't like the way that you clustered, you can recircle the pixels to recluster.  Once you've got them clustered the way you like, threshold out the small clusters by sliding the yellow horizontal line on the histogram of cluster sizes.  \n",
    "\n",
    "![cluster size threshold](images/cluster_size_threshold.png)\n",
    "\n",
    "Once you are satisfied with the clusters, press the 'Fit Gaussian' button.  This will go through each cluster and fit a 2D Gaussian curve to a square cut out of the normalized movie, to determine the origin of the event. It averages frames in the normalized movie from the first frame of that cluster to the last frame.  For a cluster that lasts 3 frames, the algorithm averages those three frames before fitting a gaussian.  \n",
    "\n",
    "Once the fitting is competed, the Puff Analyzer dialog pops up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puff Analyzer\n",
    "If you look back at the $F/F_0$ window, it will have a bunch of red dots on it.  Each one of those dots represents a detected Ca2+ signal.\n",
    "\n",
    "![Map of Event Centroids](images/puff_map.png)\n",
    "\n",
    "The locations of the dots were determined by fitting a 2D gaussian to the event.  A square region around the event is cut out, and all the frames between the start frame and end frame of the event are averaged, pixel-by-pixel.  This averaged image is then fit, and the centroid of that event is plotted as a red dot.  The figure below shows an averaged image and the resulting gaussian fit.  The accuracy of that centroid, assuming the underlying event is stationary and gaussian shaped, is described [here](http://scitru.com/articles/3/). \n",
    "\n",
    "![Gaussian Fitting](images/gaussian_fitting.PNG)\n",
    "\n",
    "If you click on the red outline of the event in the trace window, or if you click on the centroid in the $F/F_0$ window, the Puff Analyzer dialog will display the trace for that signal.  The trace is created by averaging the values of the pixels in a square centered around the centroid of the gaussian fit.  The width of the square is specified by the user (the parameter 'roi_width').  The yellow vertical lines denote the frames of the first and last pixel in the cluster.  The lower horizontal line is the amplitude of the trace at the first frame.  The horizontal line is the maximum amplitude of the trace.  The vertical lines can be adjusted by clicking and dragging.  Once adjusted, all the kinetics are recalculated for that event.  All these values are in terms of $F/F_0$, and all of them are saved in the excel spreadsheet when you press the button \"Export to Excel\".\n",
    "\n",
    "![Event Trace](images/puff_trace.PNG)\n",
    "\n",
    "Here are a few of the things you can do to explore your data:\n",
    "\n",
    "* Right click and drag inside the ROI to move it around.  Drag near a corner to change its size.\n",
    "\n",
    "* Delete all events in the ROI by pressing the delete button when your mouse is inside the ROI.\n",
    "\n",
    "* Delete a single puff by pressing the delete button when that puff is selected and your mouse is outside the ROI.\n",
    "\n",
    "* Group all the events in the ROI by pressing the 'g' button.\n",
    "\n",
    "* Ungroup by pressing the 'u' button.\n",
    "\n",
    "* To link views between the data window and any other window (such as the normalized window), right click on the ROI and select 'copy'. Then right click on the normalized window and select 'paste'. Then right click on the new ROI and select 'plot'. A line will appear that will plot the average inside that ROI for the normalized window.\n",
    "\n",
    "* Press the 'toggle groups' button to view the green group dots.  Click on one of these dots to bring up the group analyzer window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Export to Excel\n",
    "When the \"Export to Excel\" button is pressed, three sheets are made.  \n",
    "#### Event Data\n",
    "The \"Event Data\" sheet contains the information about each event.  The events are grouped into \"Groups\", each with a group number.  These groupings are determined by proximity of event centroids to one another.  If centroids are within the radius set by the argument 'radius', the events are grouped together.  \n",
    "\n",
    "Each row of the sheet represents one event.  Included in this data are the rise and fall times.  r20 is the number of frames the between the first frame of the event and the frame where the event's trace exceeded 20% of the maximum value.  r100 is the number of frames of the entire rising phase.  f80 is \"fall to 80%\" time.  etc.\n",
    "#### Group traces\n",
    "Each column of group traces represents one group, and the rows represent the average value of a square centered around the (unweighted) mean of the centroids of the events making up that group.  The width of the square is determined by the 'roi_width' argument.  \n",
    "#### Peak aligned Event Traces\n",
    "These traces are the same traces as displayed in the \"Puff Analyzer\" window.  They are aligned to their peaks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading Data into Flika\n",
    "Most researchers will want to load data that they have already analyzed. To do that, in the Puff Analyzer control panel, press the Save (.flika) button.  This will save all the information about the events (except the 3 movies created in the preprocessing step) in a .flika file which will be located in the same location and with the same basename as the original image file.  \n",
    "\n",
    "The next time you run the threshold_cluster() function using a data_window that was created from the file you already analyzed, if the 'load_flika_file' argument is set to True (it is by default), it will load that flika file and all the events will be exactly as you left them last time.  For example, the events that you trashed before will still be trashed.  You also don't need the normalized_window or the blurred_window arguments when loading the flika file, so you can pass None to these arguments.  Here is an example script which will load data I've previously analyzed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from plugins.detect_puffs.threshold_cluster import threshold_cluster\n",
    "filename=r'C:/Users/Kyle/Desktop/data.tif'\n",
    "open_file(filename)\n",
    "subtract(10) #subtract camera black level \n",
    "data_window = ratio(150,300,'average'); #ratio(first_frame, nFrames, ratio_type), now we are in F/F0\n",
    "data_window.setName('Data Window dF/F0')\n",
    "threshold_cluster(data_window, None, None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
